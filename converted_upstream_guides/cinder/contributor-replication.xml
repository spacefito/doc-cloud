<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook">
  <title>Replication</title>
  <para>For backend devices that offer replication features, Cinder provides a common
            mechanism for exposing that functionality on a per volume basis while still
            trying to allow flexibility for the varying implementation and requirements of
            all the different backend devices.</para>
  <para>There are 2 sides to Cinder’s replication feature, the core mechanism and the
            driver specific functionality, and in this document we’ll only be covering the
            driver side of things aimed at helping vendors implement this functionality in
            their drivers in a way consistent with all other drivers.</para>
  <para>Although we’ll be focusing on the driver implementation there will also be some
            mentions on deployment configurations to provide a clear picture to developers
            and help them avoid implementing custom solutions to solve things that were
            meant to be done via the cloud configuration.</para>
  <section>
    <title>Overview</title>
    <para>As a general rule replication is enabled and configured via the cinder.conf
                file under the driver’s section, and volume replication is requested through
                the use of volume types.</para>
    <para><emphasis>NOTE</emphasis>: Current replication implementation is v2.1 and it’s meant to solve a
                very specific use case, the “smoking hole” scenario.  It’s critical that you
                read the Use Cases section of the spec here:
                <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://specs.openstack.org/openstack/cinder-specs/specs/mitaka/cheesecake.html"/></para>
    <para>From a user’s perspective volumes will be created using specific volume types,
                even if it is the default volume type, and they will either be replicated or
                not, which will be reflected on the <literal>replication_status</literal> field of the volume.
                So in order to know if a snapshot is replicated we’ll have to check its volume.</para>
    <para>After the loss of the primary storage site all operations on the resources will
                fail and VMs will no longer have access to the data.  It is then when the Cloud
                Administrator will issue the <literal>failover-host</literal> command to make the
                cinder-volume service perform the failover.</para>
    <para>After the failover is completed, the Cinder volume service will start using the
                failed-over secondary storage site for all operations and the user will once
                again be able to perform actions on all resources that were replicated, while
                all other resources will be in error status since they are no longer available.</para>
  </section>
  <section>
    <title>Storage Device configuration</title>
    <para>Most storage devices will require configuration changes to enable the
                replication functionality, and this configuration process is vendor and storage
                device specific so it is not contemplated by the Cinder core replication
                functionality.</para>
    <para>It is up to the vendors whether they want to handle this device configuration
                in the Cinder driver or as a manual process, but the most common approach is to
                avoid including this configuration logic into Cinder and having the Cloud
                Administrators do a manual process following a specific guide to enable
                replication on the storage device before configuring the cinder volume service.</para>
  </section>
  <section>
    <title>Service configuration</title>
    <para>The way to enable and configure replication is common to all drivers and it is
                done via the <literal>replication_device</literal> configuration option that goes in the
                driver’s specific section in the <literal>cinder.conf</literal> configuration file.</para>
    <para><literal>replication_device</literal> is a multi dictionary option, that should be specified
                for each replication target device the admin wants to configure.</para>
    <para>While it is true that all drivers use the same <literal>replication_device</literal>
                configuration option this doesn’t mean that they will all have the same data,
                as there is only one standardized and <emphasis role="bold">REQUIRED</emphasis> key in the configuration
                entry, all others are vendor specific:</para>
    <itemizedlist>
      <listitem>
        <para>backend_id:&lt;vendor-identifier-for-rep-target&gt;</para>
      </listitem>
    </itemizedlist>
    <para>Values of <literal>backend_id</literal> keys are used to uniquely identify within the driver
                each of the secondary sites, although they can be reused on different driver
                sections.</para>
    <para>These unique identifiers will be used by the failover mechanism as well as in
                the driver initialization process, and the only requirement is that is must
                never have the value “default”.</para>
    <para>An example driver configuration for a device with multiple replication targets
                is show below:</para>
    <screen>.....
[driver-biz]
volume_driver=xxxx
volume_backend_name=biz

[driver-baz]
volume_driver=xxxx
volume_backend_name=baz

[driver-foo]
volume_driver=xxxx
volume_backend_name=foo
replication_device = backend_id:vendor-id-1,unique_key:val....
replication_device = backend_id:vendor-id-2,unique_key:val....</screen>
    <para>In this example the result of calling
                <literal>self.configuration.safe_get('replication_device)</literal> within the driver is the
                following list:</para>
    <screen>[{backend_id: vendor-id-1, unique_key: val1},
 {backend_id: vendor-id-2, unique_key: val2}]</screen>
    <para>It is expected that if a driver is configured with multiple replication
                targets, that replicated volumes are actually replicated on <emphasis role="bold">all targets</emphasis>.</para>
    <para>Besides specific replication device keys defined in the <literal>replication_device</literal>,
                a driver may also have additional normal configuration options in the driver
                section related with the replication to allow Cloud Administrators to configure
                things like timeouts.</para>
  </section>
  <section>
    <title>Capabilities reporting</title>
    <para>There are 2 new replication stats/capability keys that drivers supporting
                relication v2.1 should be reporting: <literal>replication_enabled</literal> and
                <literal>replication_targets</literal>:</para>
    <screen>stats["replication_enabled"] = True|False
stats["replication_targets"] = [&lt;backend-id_1, &lt;backend-id_2&gt;...]</screen>
    <para>If a driver is behaving correctly we can expect the <literal>replication_targets</literal>
                field to be populated whenever <literal>replication_enabled</literal> is set to <literal>True</literal>, and
                it is expected to either be set to <literal>[]</literal> or be missing altogether when
                <literal>replication_enabled</literal> is set to <literal>False</literal>.</para>
    <para>The purpose of the <literal>replication_enabled</literal> field is to be used by the scheduler
                in volume types for creation and migrations.</para>
    <para>As for the <literal>replication_targets</literal> field it is only provided for informational
                purposes so it can be retrieved through the <literal>get_capabilities</literal> using the
                admin REST API, but it will not be used for validation at the API layer.  That
                way Cloud Administrators will be able to know available secondary sites where
                they can failover.</para>
  </section>
  <section xml:id="volume-types-extra-specs">
    <title>Volume Types / Extra Specs</title>
    <para>The way to control the creation of volumes on a cloud with backends that have
                replication enabled is, like with many other features, through the use of
                volume types.</para>
    <para>We won’t go into the details of volume type creation, but suffice to say that
                you will most likely want to use volume types to discriminate between
                replicated and non replicated volumes and be explicit about it so that non
                replicated volumes won’t end up in a replicated backend.</para>
    <para>Since the driver is reporting the <literal>replication_enabled</literal> key, we just need to
                require it for replication volume types adding <literal>replication_enabled='&lt;is&gt;
True`</literal> and also specifying it for all non replicated volume types
                <literal>replication_enabled='&lt;is&gt; False'</literal>.</para>
    <para>It’s up to the driver to parse the volume type info on create and set things up
                as requested.  While the scoping key can be anything, it’s strongly recommended
                that all backends utilize the same key (replication) for consistency and to
                make things easier for the Cloud Administrator.</para>
    <para>Additional replication parameters can be supplied to the driver using vendor
                specific properties through the volume type’s extra-specs so they can be used
                by the driver at volume creation time, or retype.</para>
    <para>It is up to the driver to parse the volume type info on create and retype to
                set things up as requested.  A good pattern to get a custom parameter from a
                given volume instance is this:</para>
    <screen>extra_specs = getattr(volume.volume_type, 'extra_specs', {})
custom_param = extra_specs.get('custom_param', 'default_value')</screen>
    <para>It may seem convoluted, but we must be careful when retrieving the
                <literal>extra_specs</literal> from the <literal>volume_type</literal> field as it could be <literal>None</literal>.</para>
    <para>Vendors should try to avoid obfuscating their custom properties and expose them
                using the <literal>_init_vendor_properties</literal> method so they can be checked by the
                Cloud Administrator using the <literal>get_capabilities</literal> REST API.</para>
    <para><emphasis>NOTE</emphasis>: For storage devices doing per backend/pool replication the use of
                volume types is also recommended.</para>
  </section>
  <section>
    <title>Volume creation</title>
    <para>Drivers are expected to honor the replication parameters set in the volume type
                during creation, retyping, or migration.</para>
    <para>When implementing the replication feature there are some driver methods that
                will most likely need modifications -if they are implemented in the driver
                (since some are optional)- to make sure that the backend is replicating volumes
                that need to be replicated and not replicating those that don’t need to be:</para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>create_volume</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>create_volume_from_snapshot</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>create_cloned_volume</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>retype</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>clone_image</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>migrate_volume</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>In these methods the driver will have to check the volume type to see if the
                volumes need to be replicated, we could use the same pattern described in the
                <xref linkend="volume-types-extra-specs"/> section:</para>
    <screen>def _is_replicated(self, volume):
    specs = getattr(volume.volume_type, 'extra_specs', {})
    return specs.get('replication_enabled') == '&lt;is&gt; True'</screen>
    <para>But it is <emphasis role="bold">not</emphasis> the recommended mechanism, and the <literal>is_replicated</literal> method
                available in volumes and volume types versioned objects instances should be
                used instead.</para>
    <para>Drivers are expected to keep the <literal>replication_status</literal> field up to date and in
                sync with reality, usually as specified in the volume type.  To do so in above
                mentioned methods’ implementation they should use the update model mechanism
                provided for each one of those methods.  One must be careful since the update
                mechanism may be different from one method to another.</para>
    <para>What this means is that most of these methods should be returning a
                <literal>replication_status</literal> key with the value set to <literal>enabled</literal> in the model
                update dictionary if the volume type is enabling replication.  There is no need
                to return the key with the value of <literal>disabled</literal> if it is not enabled since
                that is the default value.</para>
    <para>In the case of the <literal>create_volume</literal>, and <literal>retype</literal> method there is no need to
                return the <literal>replication_status</literal> in the model update since it has already been
                set by the scheduler on creation using the extra spec from the volume type. And
                on <literal>migrate_volume</literal> there is no need either since there is no change to the
                <literal>replication_status</literal>.</para>
    <para><emphasis>NOTE</emphasis>: For storage devices doing per backend/pool replication it is not
                necessary to check the volume type for the <literal>replication_enabled</literal> key since
                all created volumes will be replicated, but they are expected to return the
                <literal>replication_status</literal> in all those methods, including the <literal>create_volume</literal>
                method since the driver may receive a volume creation request without the
                replication enabled extra spec and therefore the driver will not have set the
                right <literal>replication_status</literal> and the driver needs to correct this.</para>
    <para>Besides the <literal>replication_status</literal> field that drivers need to update there are
                other fields in the database related to the replication mechanism that the
                drivers can use:</para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>replication_extended_status</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>replication_driver_data</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>These fields are string type fields with a maximum size of 255 characters and
                they are available for drivers to use internally as they see fit for their
                normal replication operation.  So they can be assigned in the model update and
                later on used by the driver, for example during the failover.</para>
    <para>To avoid using magic strings drivers must use values defined by the
                <literal>ReplicationsSatus</literal> class in <literal>cinder/objects/fields.py</literal> file and
                these are:</para>
    <itemizedlist>
      <listitem>
        <para><literal>ERROR</literal>: When setting the replication failed on creation, retype, or
                        migrate.  This should be accompanied by the volume status <literal>error</literal>.</para>
      </listitem>
      <listitem>
        <para><literal>ENABLED</literal>: When the volume is being replicated.</para>
      </listitem>
      <listitem>
        <para><literal>DISABLED</literal>: When the volume is not being replicated.</para>
      </listitem>
      <listitem>
        <para><literal>FAILED_OVER</literal>: After a volume has been successfully failed over.</para>
      </listitem>
      <listitem>
        <para><literal>FAILOVER_ERROR</literal>: When there was an error during the failover of this
                        volume.</para>
      </listitem>
      <listitem>
        <para><literal>NOT_CAPABLE</literal>: When we failed-over but the volume was not replicated.</para>
      </listitem>
    </itemizedlist>
    <para>The first 3 statuses revolve around the volume creation and the last 3 around
                the failover mechanism.</para>
    <para>The only status that should not be used for the volume’s <literal>replication_status</literal>
                is the <literal>FAILING_OVER</literal> status.</para>
    <para>Whenever we are referring to values of the <literal>replication_status</literal> in this
                document we will be referring to the <literal>ReplicationStatus</literal> attributes and not a
                literal string, so <literal>ERROR</literal> means
                <literal>cinder.objects.field.ReplicationStatus.ERROR</literal> and not the string “ERROR”.</para>
  </section>
  <section xml:id="failover">
    <title>Failover</title>
    <para>This is the mechanism used to instruct the cinder volume service to fail over
                to a secondary/target device.</para>
    <para>Keep in mind the use case is that the primary backend has died a horrible death
                and is no longer valid, so any volumes that were on the primary and were not
                being replicated will no longer be available.</para>
    <para>The method definition required from the driver to implement the failback
                mechanism is as follows:</para>
    <screen>def failover_host(self, context, volumes, secondary_id=None):</screen>
    <para>There are several things that are expected of this method:</para>
    <itemizedlist>
      <listitem>
        <para>Promotion of a secondary storage device to primary</para>
      </listitem>
      <listitem>
        <para>Generating the model updates</para>
      </listitem>
      <listitem>
        <para>Changing internally to access the secondary storage device for all future
                        requests.</para>
      </listitem>
    </itemizedlist>
    <para>If no secondary storage device is provided to the driver via the <literal>backend_id</literal>
                argument (it is equal to <literal>None</literal>), then it is up to the driver to choose which
                storage device to failover to.  In this regard it is important that the driver
                takes into consideration that it could be failing over from a secondary (there
                was a prior failover request), so it should discard current target from the
                selection.</para>
    <para>If the <literal>secondary_id</literal> is not a valid one the driver is expected to raise
                <literal>InvalidReplicationTarget</literal>, for any other non recoverable errors during a
                failover the driver should raise <literal>UnableToFailOver</literal> or any child of
                <literal>VolumeDriverException</literal> class and revert to a state where the previous
                backend is in use.</para>
    <para>The failover method in the driver will receive a list of replicated volumes
                that need to be failed over.  Replicated volumes passed to the driver may have
                diverse <literal>replication_status</literal> values, but they will always be one of:
                <literal>ENABLED</literal>, <literal>FAILED_OVER</literal>, or <literal>FAILOVER_ERROR</literal>.</para>
    <para>The driver must return a 2-tuple with the new storage device target id as the
                first element and a list of dictionaries with the model updates required for
                the volumes so that the driver can perform future actions on those volumes now
                that they need to be accessed on a different location.</para>
    <para>It’s not a requirement for the driver to return model updates for all the
                volumes, or for any for that matter as it can return <literal>None</literal> or an empty list
                if there’s no update necessary.  But if elements are returned in the model
                update list then it is a requirement that each of the dictionaries contains 2
                key-value pairs, <literal>volume_id</literal> and <literal>updates</literal> like this:</para>
    <screen>[{
     'volume_id': volumes[0].id,
     'updates': {
         'provider_id': new_provider_id1,
         ...
     },
     'volume_id': volumes[1].id,
     'updates': {
         'provider_id': new_provider_id2,
         'replication_status': fields.ReplicationStatus.FAILOVER_ERROR,
         ...
     },
}]</screen>
    <para>In these updates there is no need to set the <literal>replication_status</literal> to
                <literal>FAILED_OVER</literal> if the failover was successful, as this will be performed by
                the manager by default, but it won’t create additional DB queries if it is
                returned.  It is however necessary to set it to <literal>FAILOVER_ERROR</literal> for those
                volumes that had errors during the failover.</para>
    <para>Driver’s don’t have to worry about snapshots or non replicated volumes, since
                the manager will take care of those in the following manner:</para>
    <itemizedlist>
      <listitem>
        <para>All non replicated volumes will have their current <literal>status</literal> field saved in
                        the <literal>previous_status</literal> field, the <literal>status</literal> field changed to <literal>error</literal>, and
                        their <literal>replication_status</literal> set to <literal>NOT_CAPABLE</literal>.</para>
      </listitem>
      <listitem>
        <para>All snapshots from non replicated volumes will have their statuses changed to
                        <literal>error</literal>.</para>
      </listitem>
      <listitem>
        <para>All replicated volumes that failed on the failover will get their <literal>status</literal>
                        changed to <literal>error</literal>, their current <literal>status</literal> preserved in
                        <literal>previous_status</literal>, and their <literal>replication_status</literal> set to
                        <literal>FAILOVER_ERROR</literal> .</para>
      </listitem>
      <listitem>
        <para>All snapshots from volumes that had errors during the failover will have
                        their statuses set to <literal>error</literal>.</para>
      </listitem>
    </itemizedlist>
    <para>Any model update request from the driver that changes the <literal>status</literal> field will
                trigger a change in the <literal>previous_status</literal> field to preserve the current
                status.</para>
    <para>Once the failover is completed the driver should be pointing to the secondary
                and should be able to create and destroy volumes and snapshots as usual, and it
                is left to the Cloud Administrator’s discretion whether resource modifying
                operations are allowed or not.</para>
  </section>
  <section>
    <title>Failback</title>
    <para>Drivers are not required to support failback, but they are required to raise a
                <literal>InvalidReplicationTarget</literal> exception if the failback is requested but not
                supported.</para>
    <para>The way to request the failback is quite simple, the driver will receive the
                argument <literal>secondary_id</literal> with the value of <literal>default</literal>.  That is why if was
                forbidden to use the <literal>default</literal> on the target configuration in the cinder
                configuration file.</para>
    <para>Expected driver behavior is the same as the one explained in the <xref linkend="failover"/>
                section:</para>
    <itemizedlist>
      <listitem>
        <para>Promotion of the original primary to primary</para>
      </listitem>
      <listitem>
        <para>Generating the model updates</para>
      </listitem>
      <listitem>
        <para>Changing internally to access the original primary storage device for all
                        future requests.</para>
      </listitem>
    </itemizedlist>
    <para>If the failback of any of the volumes fail the driver must return
                <literal>replication_status</literal> set to <literal>ERROR</literal> in the volume updates for those
                volumes.  If they succeed it is not necessary to change the
                <literal>replication_status</literal> since the default behavior will be to set them to
                <literal>ENABLED</literal>, but it won’t create additional DB queries if it is set.</para>
    <para>The manager will update resources in a slightly different way than in the
                failover case:</para>
    <itemizedlist>
      <listitem>
        <para>All non replicated volumes will not have any model modifications.</para>
      </listitem>
      <listitem>
        <para>All snapshots from non replicated volumes will not have any model
                        modifications.</para>
      </listitem>
      <listitem>
        <para>All replicated volumes that failed on the failback will get their <literal>status</literal>
                        changed to <literal>error</literal>, have their current <literal>status</literal> preserved in the
                        <literal>previous_status</literal> field, and their <literal>replication_status</literal> set to
                        <literal>FAILOVER_ERROR</literal>.</para>
      </listitem>
      <listitem>
        <para>All snapshots from volumes that had errors during the failover will have
                        their statuses set to <literal>error</literal>.</para>
      </listitem>
    </itemizedlist>
    <para>We can avoid using the “default” magic string by using the
                <literal>FAILBACK_SENTINEL</literal> class attribute from the <literal>VolumeManager</literal> class.</para>
  </section>
  <section>
    <title>Initialization</title>
    <para>It stands to reason that a failed over Cinder volume service may be restarted,
                so there needs to be a way for a driver to know on start which storage device
                should be used to access the resources.</para>
    <para>So, to let drivers know which storage device they should use the manager passes
                drivers the <literal>active_backend_id</literal> argument to their <literal>__init__</literal> method during
                the initialization phase of the driver.  Default value is <literal>None</literal> when the
                default (primary) storage device should be used.</para>
    <para>Drivers should store this value if they will need it, as the base driver is not
                storing it, for example to determine the current storage device when a failover
                is requested and we are already in a failover state, as mentioned above.</para>
  </section>
  <section>
    <title>Freeze / Thaw</title>
    <para>In many cases, after a failover has been completed we’ll want to allow changes
                to the data in the volumes as well as some operations like attach and detach
                while other operations that modify the number of existing resources, like
                delete or create, are not allowed.</para>
    <para>And that is where the freezing mechanism comes in; freezing a backend puts the
                control plane of the specific Cinder volume service into a read only state, or
                at least most of it, while allowing the data plane to proceed as usual.</para>
    <para>While this will mostly be handled by the Cinder core code, drivers are informed
                when the freezing mechanism is enabled or disabled via these 2 calls:</para>
    <screen>freeze_backend(self, context)
thaw_backend(self, context)</screen>
    <para>In most cases the driver may not need to do anything, and then it doesn’t need
                to define any of these methods as long as its a child class of the <literal>BaseVD</literal>
                class that already implements them as noops.</para>
    <para>Raising a <literal>VolumeDriverException</literal> exception in any of these methods will result
                in a 500 status code response being returned to the caller and the manager will
                not log the exception, so it’s up to the driver to log the error if it is
                appropriate.</para>
    <para>If the driver wants to give a more meaningful error response, then it can raise
                other exceptions that have different status codes.</para>
    <para>When creating the <literal>freeze_backend</literal> and <literal>thaw_backend</literal> driver methods we must
                remember that this is a Cloud Administrator operation, so we can return errors
                that reveal internals of the cloud, for example the type of storage device, and
                we must use the appropriate internationalization translation methods when
                raising exceptions; for <literal>VolumeDriverException</literal> no translation is necessary
                since the manager doesn’t log it or return to the user in any way, but any
                other exception should use the <literal>_()</literal> translation method since it will be
                returned to the REST API caller.</para>
    <para>For example, if a storage device doesn’t support the thaw operation when failed
                over, then it should raise an <literal>Invalid</literal> exception:</para>
    <screen>def thaw_backend(self, context):
    if self.failed_over:
        msg = _('Thaw is not supported by driver XYZ.')
        raise exception.Invalid(msg)</screen>
  </section>
</section>
